{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:57:17,248 - llmbo.llmbo.StructuredBatchInferer - INFO - Attempting to Recover BatchInferer from arn:aws:bedrock:us-east-1:992382722318:model-invocation-job/7whskxnh15zc\n",
      "2025-02-17 16:57:17,275 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-02-17 16:57:31,679 - llmbo.llmbo.StructuredBatchInferer - INFO - Initialized StructuredBatchInferer with TaskOutput schema\n",
      "2025-02-17 16:57:31,679 - llmbo.llmbo.StructuredBatchInferer - INFO - Intialising BatchInferer\n",
      "2025-02-17 16:57:31,686 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-02-17 16:57:45,853 - llmbo.llmbo.StructuredBatchInferer - INFO - Role 'BatchInferenceRole' exists.\n",
      "2025-02-17 16:57:45,872 - llmbo.llmbo.StructuredBatchInferer - INFO - Initialized BatchInferer\n",
      "2025-02-17 16:57:46,170 - llmbo.llmbo.StructuredBatchInferer - INFO - Job arn:aws:bedrock:us-east-1:992382722318:model-invocation-job/7whskxnh15zc is already Completed\n",
      "2025-02-17 16:57:46,170 - llmbo.llmbo.StructuredBatchInferer - INFO - Job:arn:aws:bedrock:us-east-1:992382722318:model-invocation-job/7whskxnh15zc Complete. Downloading results from cddo-af-bedrock-batch-inference-us-east-1\n",
      "2025-02-17 16:57:46,846 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,939 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,956 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,959 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,961 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,963 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,971 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,976 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,990 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:46,991 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:48,437 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:48,537 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:51,815 - llmbo.llmbo.StructuredBatchInferer - INFO - Downloaded results file to hmrc-task-extraction-20250212-104421_out.jsonl\n",
      "2025-02-17 16:57:52,026 - botocore.httpchecksum - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-17 16:57:52,028 - llmbo.llmbo.StructuredBatchInferer - INFO - Downloaded manifest file to hmrc-task-extraction-20250212-104421_manifest.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23233\n"
     ]
    }
   ],
   "source": [
    "#  This cell loads the batch job from bedrock. It returns the extracted tasks for each\n",
    "#  job.\n",
    "from llmbo import StructuredBatchInferer\n",
    "from extraction import TaskOutput\n",
    "\n",
    "\n",
    "sbi = StructuredBatchInferer.recover_structured_job(\n",
    "    job_arn=\"arn:aws:bedrock:us-east-1:992382722318:model-invocation-job/7whskxnh15zc\",\n",
    "    region=\"us-east-1\",\n",
    "    output_model=TaskOutput,\n",
    "    session=boto3.Session()\n",
    ")\n",
    "\n",
    "sbi.download_results()\n",
    "sbi.load_results()\n",
    "\n",
    "print(len(sbi.instances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the jobs dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "hmrc_jobs = pd.read_parquet(\"./data/jobs.pq\")\n",
    "hmrc_jobs = hmrc_jobs.loc[hmrc_jobs[\"department\"] == \"HM Revenue and Customs\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  convert the instances from the batch processor into a data frame\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tasks_dict = {}\n",
    "for item in sbi.instances:\n",
    "    if item:\n",
    "        # Extract vacancy_id from the recordId string\n",
    "        vacancy_id = int(item[\"recordId\"].split(\"=\")[1])\n",
    "        # Get the tasks from the outputModel\n",
    "        if item[\"outputModel\"]:\n",
    "            tasks = item[\"outputModel\"].model_dump()[\"tasks\"]\n",
    "            tasks_dict[vacancy_id] = tasks\n",
    "\n",
    "\n",
    "tasks_df = pd.DataFrame(\n",
    "    [{\"vacancy_id\": key, **item} for key, tasks in tasks_dict.items() for item in tasks]\n",
    ").set_index(\"vacancy_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement the job listings with the llm output task and exposure score.\n",
    "hmrc_jobs_with_tasks = hmrc_jobs.merge(tasks_df, on=\"vacancy_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This computes the summary stats for each vacancy, i.e. how exposed the role is to automation\n",
    "hmrc_stats = hmrc_jobs_with_tasks.groupby(\"vacancy_id\")[\"exposure_score\"].agg(\n",
    "    [\"count\", \"mean\", \"median\", \"std\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmrc_with_stats = hmrc_jobs.merge(hmrc_stats, how=\"left\", on=\"vacancy_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distribution plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=hmrc_stats[\"mean\"], kde=True)\n",
    "plt.title(\"Distribution of Mean Exposure Scores\")\n",
    "plt.xlabel(\"Mean Exposure Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concentrating on the roles where their is a high degree of automation potential\n",
    "high_automation_exposure = hmrc_jobs.merge(\n",
    "    hmrc_stats[hmrc_stats[\"mean\"] >= 0.7], how=\"right\", on=\"vacancy_id\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{len(high_automation_exposure)} of {len(hmrc_jobs)} are HIGH\")\n",
    "print(\n",
    "    f\"{len(high_automation_exposure['vacancy_title'].str.lower().unique())} vacancy_titles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a longish list of job titles, use an llm to uncover the groups.\n",
    "from mirascope.core import Messages, bedrock\n",
    "import boto3\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "taxonomy_prompt = \"\"\"\n",
    "You are an expert in creating groups of similar job titles. You examine the full list of \n",
    "job titles. Create a list of groups which cover the full list. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class JobTitleTaxonomy(BaseModel):\n",
    "    categories: List[str] = Field(..., description=\"A list of the job categories\")\n",
    "\n",
    "\n",
    "@bedrock.call(\n",
    "    \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    call_params={\"temperature\": 0.2, \"max_tokens\": 8000},\n",
    "    client=boto3.Session().client(\"bedrock-runtime\", region_name=\"us-east-1\"),\n",
    "    response_model=JobTitleTaxonomy,\n",
    ")  # type: ignore\n",
    "def map_jobs(job_titles: str) -> Messages.Type:\n",
    "    return [\n",
    "        Messages.System(taxonomy_prompt),\n",
    "        Messages.User(f\"\"\"\n",
    "            Group these job titles:  \n",
    "            <job_titles>{job_titles}</job_titles>\n",
    "            \"\"\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "job_groupings = map_jobs(\n",
    "    \"\\n\".join(high_automation_exposure[\"vacancy_title\"].str.lower().unique())\n",
    ")\n",
    "\n",
    "job_groupings.categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the model has created a taxonomy/groupings of the roles. Use those and map\n",
    "# each of the roles.\n",
    "\n",
    "from enum import Enum\n",
    "from llmbo import ModelInput\n",
    "from datetime import datetime\n",
    "\n",
    "# # Create the Enum dynamically\n",
    "# Category = Enum(\"JobCategories\", job_groupings.categories)\n",
    "\n",
    "Category = Enum(\"JobCategories\", ['Data & Analytics',\n",
    " 'Administrative & Support',\n",
    " 'Finance & Accounting',\n",
    " 'Project & Program Management',\n",
    " 'Customer Service',\n",
    " 'Technical & IT',\n",
    " 'HR & Recruitment',\n",
    " 'Compliance & Risk',\n",
    " 'Management & Leadership',\n",
    " 'Legal & Policy',\n",
    " 'Communications & Marketing',\n",
    " 'Operations'])\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in mapping job titles to categories provided\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prompt(job_description):\n",
    "    return f\"\"\"\n",
    "    Map a single job title to the closest category provied.\n",
    "    <job_title>{job_description}</job_title>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class Mapper(BaseModel):\n",
    "    mapped_category: Category\n",
    "\n",
    "\n",
    "try:\n",
    "    job_mapper_batcher = StructuredBatchInferer.recover_structured_job(\n",
    "        job_arn=\"arn:aws:bedrock:us-east-1:992382722318:model-invocation-job/st34z39z3ckv\",\n",
    "        region=\"us-east-1\",\n",
    "        output_model=Mapper,\n",
    "    )\n",
    "\n",
    "    job_mapper_batcher.download_results()\n",
    "    job_mapper_batcher.load_results()\n",
    "except ValueError:\n",
    "    # modify this to use a better model.\n",
    "    job_mapper_batcher = StructuredBatchInferer(\n",
    "        output_model=Mapper,\n",
    "        model_name=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        region=\"us-east-1\",\n",
    "        job_name=f\"hmrc-job-mapper-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "        bucket_name=\"cddo-af-bedrock-batch-inference-us-east-1\",\n",
    "        role_arn=\"arn:aws:iam::992382722318:role/BatchInferenceRole\",\n",
    "    )\n",
    "    inputs = {\n",
    "        f\"vacancy_id={row.vacancy_id}\": ModelInput(\n",
    "            system=SYSTEM_PROMPT,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt(row.job_description)}],\n",
    "        )\n",
    "        for row in high_automation_exposure.itertuples()\n",
    "    }\n",
    "\n",
    "    job_mapper_batcher.prepare_requests(inputs)\n",
    "    job_mapper_batcher.push_requests_to_s3()\n",
    "    job_mapper_batcher.create()\n",
    "    job_mapper_batcher.poll_progress(60)\n",
    "\n",
    "    job_mapper_batcher.download_results()\n",
    "    job_mapper_batcher.load_results()\n",
    "\n",
    "\n",
    "print(job_mapper_batcher.manifest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_job_titles = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"vacancy_id\": int(item[\"recordId\"].split(\"=\")[1]),  # Extract ID\n",
    "            \"category\": item[\"outputModel\"].mapped_category.name,\n",
    "        }\n",
    "        for item in job_mapper_batcher.instances\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "high_automation_exposure = high_automation_exposure.merge(\n",
    "    mapped_job_titles, on=\"vacancy_id\", how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics for each job group\n",
    "summary = (\n",
    "    high_automation_exposure.groupby(\"category\")\n",
    "    .apply(\n",
    "        lambda g: pd.Series(\n",
    "            {\n",
    "                \"count\": g[\"count\"].sum(),\n",
    "                \"mean\": np.average(g[\"mean\"], weights=g[\"count\"]),  # Weighted mean\n",
    "                \"std\": np.sqrt(\n",
    "                    np.sum(\n",
    "                        (g[\"count\"] - 1) * (g[\"std\"] ** 2)\n",
    "                        + g[\"count\"]\n",
    "                        * (g[\"mean\"] - np.average(g[\"mean\"], weights=g[\"count\"])) ** 2\n",
    "                    )\n",
    "                    / (g[\"count\"].sum() - 1)  # Pooled standard deviation formula\n",
    "                ),\n",
    "            }\n",
    "        ),\n",
    "    include_groups=False)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(\n",
    "    summary,\n",
    "    x=\"mean\",\n",
    "    y=\"std\",\n",
    "    # text=\"category\",\n",
    "    # size=\"count\", \n",
    "    hover_data={\n",
    "        \"category\": True,\n",
    "        \"count\": True,\n",
    "        \"mean\": True,\n",
    "        \"std\": True,\n",
    "    },  # Display extra details\n",
    "    title=\"Mean vs Standard Deviation by Job Category\",\n",
    "    labels={\"mean\": \"Mean\", \"std\": \"Standard Deviation\"},\n",
    ")\n",
    "\n",
    "# Improve layout\n",
    "fig.update_traces(marker=dict(opacity=0.7, line=dict(width=1, color=\"black\")))\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Mean\"), yaxis=dict(title=\"Standard Deviation\"), showlegend=False\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(\n",
    "    high_automation_exposure,\n",
    "    x=\"mean\",\n",
    "    y=\"std\",\n",
    "    color=\"category\",\n",
    "    # text=\"category\",\n",
    "    # size=\"count\", \n",
    "    hover_data={\n",
    "        \"category\": True,\n",
    "        \"count\": True,\n",
    "        \"mean\": True,\n",
    "        \"std\": True,\n",
    "    },  # Display extra details\n",
    "    title=\"Mean vs Standard Deviation by Job Category\",\n",
    "    labels={\"mean\": \"Mean\", \"std\": \"Standard Deviation\"},\n",
    ")\n",
    "\n",
    "# Improve layout\n",
    "fig.update_traces(marker=dict(opacity=0.7, line=dict(width=1, color=\"black\")))\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Mean\"), yaxis=dict(title=\"Standard Deviation\"), showlegend=False\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(\n",
    "    hmrc_with_stats,\n",
    "    x=\"mean\",\n",
    "    y=\"std\",\n",
    "    color=hmrc_with_stats[\"mean\"] >= 0.7,\n",
    "    # text=\"category\",\n",
    "    # size=\"count\", \n",
    "    hover_data={\n",
    "        \"vacancy_id\": True,\n",
    "        \"vacancy_title\":True,\n",
    "        \"count\": True,\n",
    "        \"mean\": True,\n",
    "        \"std\": True,\n",
    "    },  # Display extra details\n",
    "    title=\"Mean vs Standard Deviation by Job Category\",\n",
    "    labels={\"mean\": \"Mean\", \"std\": \"Standard Deviation\"},\n",
    ")\n",
    "\n",
    "# Improve layout\n",
    "fig.update_traces(marker=dict(opacity=0.7, line=dict(width=1, color=\"black\")))\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Mean\"), yaxis=dict(title=\"Standard Deviation\"), showlegend=False\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
